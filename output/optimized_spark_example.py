"""
Optimized PySpark code generated by Spark MCP.

Optimization Details:
-------------------
1. Removing unnecessary caching: In the original code, `df_users` and `df_transactions` were cached without a specific need. Caching can be beneficial when DataFrames are reused multiple times, but unnecessary caching can lead to overhead in terms of memory usage and cache management. By removing the caching statements in the optimized code, we avoid this overhead and improve overall performance.
2. Removing unnecessary transformations: The original code included unnecessary transformations like adding a dummy column, exploding the name column, and using an inefficient UDF for parsing the amount. These transformations added extra computation and memory overhead without providing any benefit to the final result. By removing these unnecessary transformations in the optimized code, we streamline the data processing pipeline and improve performance.
3. Removing unnecessary repartitioning: The original code included an unnecessary repartitioning step (`df_joined.repartition(10)`). Repartitioning can be useful when dealing with data skew or optimizing for downstream operations, but in this case, it added an extra shuffle step without any clear benefit. By removing the repartitioning in the optimized code, we avoid the additional shuffle and improve performance.
4. Filtering after aggregation: In the optimized code, the filter on the `total` column is applied after the aggregation. This allows the filter to be applied on the aggregated data, which is typically smaller in size compared to the raw data. By filtering after aggregation, we reduce the amount of data processed in the subsequent steps and improve overall performance.
5. Development time: Optimizing the code requires careful analysis and understanding of the data processing pipeline. It may take additional development time to identify and implement the optimizations compared to writing the original code. However, the performance benefits and resource optimizations achieved through the optimized code can outweigh the initial development overhead in the long run.
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, explode, sum as _sum

# Start Spark session
spark = SparkSession.builder.appName("OptimizedExample").getOrCreate()

# Sample user data
users_data = [
    ("u1", "Alice Johnson", "US"),
    ("u2", "Bob Smith", "US"),
    ("u3", "Carlos Vega", "MX"),
    ("u4", "Dana Lee", "US")
]

# Sample transaction data
transactions_data = [
    ("t1", "u1", 100.50),
    ("t2", "u2", 200.00),
    ("t3", "u1", 350.75),
    ("t4", "u3", 80.00),
    ("t5", "u4", 150.00),
    ("t6", "u4", 250.00)
]

# Create DataFrames
df_users = spark.createDataFrame(users_data, ["user_id", "full_name", "country"])
df_transactions = spark.createDataFrame(transactions_data, ["txn_id", "user_id", "amount"])

# Filter transactions before join
df_transactions_filtered = df_transactions.filter(col("user_id").isin(["u1", "u2", "u4"]))

# Join filtered transactions with users
df_joined = df_transactions_filtered.join(df_users, "user_id", "inner")

# Group and aggregate
df_agg = df_joined.groupBy("user_id", "country").agg(_sum("amount").alias("total"))

# Filter aggregated data
df_filtered = df_agg.filter(col("total") > 200)

# Select required columns
df_result = df_filtered.select(
    col("user_id").alias("uid"),
    col("total").alias("total_spent"),
    col("country")
)

# Collect to driver (not good for large data)
result = df_result.collect()

# Print output
for row in result:
    print(f"User: {row['uid']}, Country: {row['country']}, Total Spent: {row['total_spent']}")
