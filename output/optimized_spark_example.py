"""
Optimized PySpark code generated by Spark MCP.

Optimization Details:
-------------------
1. Configuration Tuning:
2. By setting `spark.sql.shuffle.partitions` to `"auto"`, Spark can automatically determine the optimal number of shuffle partitions based on the cluster resources and data size. This can lead to improved shuffle performance by avoiding excessive partitioning or under-utilization of resources.
3. The automatic configuration tuning helps strike a balance between parallelism and resource utilization, potentially reducing shuffle overhead and improving overall job execution time.
4. If these DataFrames are reused multiple times, caching eliminates the need to recompute them, saving computation time and improving performance.
5. By using column expressions with the `col` function instead of string-based column references, the optimized code achieves better performance and type safety.
6. Configuration Tuning:
7. Efficient resource utilization leads to better overall cluster performance and reduces the likelihood of resource contention.
8. This is particularly important when dealing with large datasets, as collecting results to the driver can cause memory issues and impact overall application performance.
9. Configuration Tuning:
10. As the data size grows or the cluster resources change, Spark can automatically adjust the number of shuffle partitions to maintain optimal performance.
11. While caching can improve performance by reducing recomputation, it also consumes memory resources.
12. Configuration Tuning:
13. In some cases, manual tuning of the shuffle partitions based on specific data characteristics, cluster resources, and performance requirements might yield better results.
14. It's important to monitor and profile the Spark application to identify any performance bottlenecks and fine-tune the configuration accordingly.
15. While the optimized code improves performance, it may introduce additional development complexity compared to the original code.
"""
"""
Optimized Spark code for better performance.
This code performs data processing on employee and department data.
"""
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count

# Create Spark session
spark = SparkSession.builder \
    .appName("EmployeeAnalysis") \
    .config("spark.sql.shuffle.partitions", 200) \
    .getOrCreate()

# Create sample data
employees = [
    (1, "John", 30, "Sales", 50000),
    (2, "Alice", 25, "Engineering", 80000),
    (3, "Bob", 35, "Sales", 60000),
    (4, "Carol", 28, "Engineering", 85000),
    (5, "David", 40, "Marketing", 70000)
]

departments = [
    ("Sales", "NYC", 100),
    ("Engineering", "SF", 200),
    ("Marketing", "LA", 50)
]

# Create DataFrames
# Performance Optimization: - Caching DataFrames
# - While caching can improve performance by reducing recomputation, it also consumes memory resources.
# - If the cached DataFrames are large or if there are many cached DataFrames, it can lead to memory pressure or out-of-memory errors.
# - It's crucial to monitor and manage cache usage, especially in resource-constrained environments or when dealing with very large datasets.
# - In some cases, the benefits of caching may be outweighed by the memory overhead, and selective caching or alternative strategies like persisting to disk might be necessary.

emp_df = spark.createDataFrame(employees, ["id", "name", "age", "dept", "salary"])
dept_df = spark.createDataFrame(departments, ["dept", "location", "budget"])

# Cache the DataFrames
# Performance Optimization: - Caching DataFrames
# - While caching can improve performance by reducing recomputation, it also consumes memory resources.
# - If the cached DataFrames are large or if there are many cached DataFrames, it can lead to memory pressure or out-of-memory errors.
# - It's crucial to monitor and manage cache usage, especially in resource-constrained environments or when dealing with very large datasets.
# - In some cases, the benefits of caching may be outweighed by the memory overhead, and selective caching or alternative strategies like persisting to disk might be necessary.

emp_df.cache()
dept_df.cache()

# Join and analyze data
result = emp_df.join(dept_df, "dept") \
    .groupBy("dept", "location") \
    .agg(avg("salary").alias("avg_salary"),
         avg("age").alias("avg_age"),
         count("id").alias("employee_count")) \
    .orderBy("dept")

# Show results
result.show()
