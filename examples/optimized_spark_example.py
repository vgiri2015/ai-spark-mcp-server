"""
Optimized PySpark code generated by Spark MCP.
"""
from pyspark.sql import functions as F
from pyspark.sql.window import Window

# Create employee and department DataFrames
employees = [
    (1, "John", 30, "Sales", 50000),
    (2, "Alice", 25, "Engineering", 80000),
    (3, "Bob", 35, "Sales", 60000),
    (4, "Carol", 28, "Engineering", 85000),
    (5, "David", 40, "Marketing", 70000)
]
departments = [
    ("Sales", "NYC", 100),
    ("Engineering", "SF", 200),
    ("Marketing", "LA", 50)
]

emp_df = spark.createDataFrame(employees, ["id", "name", "age", "dept", "salary"])
dept_df = spark.createDataFrame(departments, ["dept", "location", "budget"])

# Broadcast the smaller DataFrame (dept_df) for efficient join
broadcast_dept_df = F.broadcast(dept_df)

# Define window specification outside the main query
window_spec = Window.partitionBy("dept")

# Perform complex analysis with joins, window functions, and aggregations
result = (
    emp_df.join(broadcast_dept_df, "dept")
    .withColumn("avg_dept_salary", F.avg("salary").over(window_spec))
    .withColumn("salary_vs_avg", F.col("salary") - F.col("avg_dept_salary"))
    .groupBy("dept", "location")
    .agg(
        F.count("id").alias("emp_count"),
        F.sum("salary").alias("total_salary"),
        F.avg("salary_vs_avg").alias("avg_salary_diff")
    )
)

# Cache the result DataFrame and materialize it
result.cache().count()

# Show the final result
result.orderBy(F.desc("total_salary")).show()
